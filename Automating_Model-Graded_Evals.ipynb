{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import  get_circle_api_key\n",
    "cci_api_key = get_circle_api_key()\n",
    "\n",
    "from utils import get_gh_api_key\n",
    "gh_api_key = get_gh_api_key()\n",
    "\n",
    "from utils import get_openai_api_key\n",
    "openai_api_key = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up our github branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_repo_name\n",
    "my_repo = get_repo_name()\n",
    "\n",
    "from utils import get_branch\n",
    "my_branch = get_branch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first model graded eval\n",
    "build a prompt that tells the LLM to evaluate the output of the quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "\n",
    "eval_system_prompt = f\"\"\"You are an assistant that evaluates \\\n",
    "  whether or not an assistant is producing valid quizzes.\n",
    "  The assistant should be producing output in the \\\n",
    "  format of Question N:{delimiter} <question N>?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate LLM response for the evaluation(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = \"\"\"\n",
    "Question 1:#### What is the largest telescope in space called and what material is its mirror made of?\n",
    "\n",
    "Question 2:#### True or False: Water slows down the speed of light.\n",
    "\n",
    "Question 3:#### What did Marie and Pierre Curie discover in Paris?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the prompt for evaluation(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_user_message = f\"\"\"You are evaluating a generated quiz \\\n",
    "based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {llm_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like \\\n",
    "a quiz or test. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, \\\n",
    "output N if the response does not look like a quiz.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Langchain to build the prompt template for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", eval_system_prompt),\n",
    "      (\"human\", eval_user_message),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From langchain import a parser to have a readable response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parser\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain = eval_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the 'good LLM' with positive response by invoking the eval_chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function 'create_eval_chain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_chain(\n",
    "    agent_response,\n",
    "    llm = llm,\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "  delimiter = \"####\"\n",
    "  eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
    "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
    "  \n",
    "  eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {agent_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
    "\"\"\"\n",
    "  eval_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", eval_system_prompt),\n",
    "      (\"human\", eval_user_message),\n",
    "  ])\n",
    "\n",
    "  return eval_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cretae new response to test in the eval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_bad_result = \"There are lots of interesting facts. Tell me more about what you'd like to know\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_eval_chain = create_eval_chain(known_bad_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response for wrong prompt\n",
    "bad_eval_chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new create_eval_chain into the 'test_assistant.py' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from app import assistant_chain\n",
      "from app import system_message\n",
      "from langchain.prompts                import ChatPromptTemplate\n",
      "from langchain.chat_models            import ChatOpenAI\n",
      "from langchain.schema.output_parser   import StrOutputParser\n",
      "\n",
      "import os\n",
      "\n",
      "from dotenv import load_dotenv, find_dotenv\n",
      "_ = load_dotenv(find_dotenv())\n",
      "\n",
      "def eval_expected_words(\n",
      "    system_message,\n",
      "    question,\n",
      "    expected_words,\n",
      "    human_template=\"{question}\",\n",
      "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
      "    output_parser=StrOutputParser()):\n",
      "\n",
      "  assistant = assistant_chain(system_message)\n",
      "  answer = assistant.invoke({\"question\": question})\n",
      "  print(answer)\n",
      "    \n",
      "  assert any(word in answer.lower() \\\n",
      "             for word in expected_words), \\\n",
      "    f\"Expected the assistant questions to include \\\n",
      "    '{expected_words}', but it did not\"\n",
      "\n",
      "def evaluate_refusal(\n",
      "    system_message,\n",
      "    question,\n",
      "    decline_response,\n",
      "    human_template=\"{question}\", \n",
      "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
      "    output_parser=StrOutputParser()):\n",
      "    \n",
      "  assistant = assistant_chain(human_template, \n",
      "                              system_message,\n",
      "                              llm,\n",
      "                              output_parser)\n",
      "  \n",
      "  answer = assistant.invoke({\"question\": question})\n",
      "  print(answer)\n",
      "  \n",
      "  assert decline_response.lower() in answer.lower(), \\\n",
      "    f\"Expected the bot to decline with \\\n",
      "    '{decline_response}' got {answer}\"\n",
      "\n",
      "\"\"\"\n",
      "  Test cases\n",
      "\"\"\"\n",
      "\n",
      "def test_science_quiz():\n",
      "  \n",
      "  question  = \"Generate a quiz about science.\"\n",
      "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
      "  eval_expected_words(\n",
      "      system_message,\n",
      "      question,\n",
      "      expected_subjects)\n",
      "\n",
      "def test_geography_quiz():\n",
      "  question  = \"Generate a quiz about geography.\"\n",
      "  expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
      "  eval_expected_words(\n",
      "      system_message,\n",
      "      question,\n",
      "      expected_subjects)\n",
      "\n",
      "def test_refusal_rome():\n",
      "  question  = \"Help me create a quiz about Rome\"\n",
      "  decline_response = \"I'm sorry\"\n",
      "  evaluate_refusal(\n",
      "      system_message,\n",
      "      question,\n",
      "      decline_response)\n"
     ]
    }
   ],
   "source": [
    "!type test_assistant.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_release_evals.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_release_evals.py\n",
    "\n",
    "from app import assistant_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import pytest\n",
    "\n",
    "\n",
    "def create_eval_chain(\n",
    "    agent_response,\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    output_parser=StrOutputParser(),\n",
    "):\n",
    "    delimiter = \"####\"\n",
    "    eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
    "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
    "\n",
    "    eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {agent_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
    "\"\"\"\n",
    "    eval_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", eval_system_prompt),\n",
    "            (\"human\", eval_user_message),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return eval_prompt | llm | output_parser\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def known_bad_result():\n",
    "    return \"There are lots of interesting facts. Tell me more about what you'd like to know\"\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def quiz_request():\n",
    "    return \"Give me a quiz about Geography\"\n",
    "\n",
    "\n",
    "def test_model_graded_eval(quiz_request):\n",
    "    assistant = assistant_chain()\n",
    "    result = assistant.invoke({\"question\": quiz_request})\n",
    "    print(result)\n",
    "    eval_agent = create_eval_chain(result)\n",
    "    eval_response = eval_agent.invoke({})\n",
    "    assert eval_response == \"Y\"\n",
    "\n",
    "\n",
    "def test_model_graded_eval_should_fail(known_bad_result):\n",
    "    print(known_bad_result)\n",
    "    eval_agent = create_eval_chain(known_bad_result)\n",
    "    eval_response = eval_agent.invoke({})\n",
    "    assert (\n",
    "        eval_response == \"Y\"\n",
    "    ), f\"expected failure, asserted the response should be 'Y', \\\n",
    "    got back '{eval_response}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from app import assistant_chain\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.schema.output_parser import StrOutputParser\n",
      "import pytest\n",
      "\n",
      "\n",
      "def create_eval_chain(\n",
      "    agent_response,\n",
      "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
      "    output_parser=StrOutputParser(),\n",
      "):\n",
      "    delimiter = \"####\"\n",
      "    eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
      "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
      "\n",
      "    eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
      "  Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Response]: {agent_response}\n",
      "    ************\n",
      "    [END DATA]\n",
      "\n",
      "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
      "only evaluate if the data is in the expected format.\n",
      "\n",
      "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
      "\"\"\"\n",
      "    eval_prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            (\"system\", eval_system_prompt),\n",
      "            (\"human\", eval_user_message),\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    return eval_prompt | llm | output_parser\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def known_bad_result():\n",
      "    return \"There are lots of interesting facts. Tell me more about what you'd like to know\"\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def quiz_request():\n",
      "    return \"Give me a quiz about Geography\"\n",
      "\n",
      "\n",
      "def test_model_graded_eval(quiz_request):\n",
      "    assistant = assistant_chain()\n",
      "    result = assistant.invoke({\"question\": quiz_request})\n",
      "    print(result)\n",
      "    eval_agent = create_eval_chain(result)\n",
      "    eval_response = eval_agent.invoke({})\n",
      "    assert eval_response == \"Y\"\n",
      "\n",
      "\n",
      "def test_model_graded_eval_should_fail(known_bad_result):\n",
      "    print(known_bad_result)\n",
      "    eval_agent = create_eval_chain(known_bad_result)\n",
      "    eval_response = eval_agent.invoke({})\n",
      "    assert (\n",
      "        eval_response == \"Y\"\n",
      "    ), f\"expected failure, asserted the response should be 'Y', \\\n",
      "    got back '{eval_response}'\"\n"
     ]
    }
   ],
   "source": [
    "# Command to see the content of the file\n",
    "!type test_release_evals.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "push the code into CircleCI's git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import push_files\n",
    "push_files(my_repo, my_branch, [\"app.py\",\"test_release_evals.py\", \"test_assistant.py\"], config=\"circle_config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger the release evalauations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import trigger_release_evals\n",
    "trigger_release_evals(my_repo, my_branch, [\"app.py\",\"test_release_evals.py\", \"test_assistant.py\"], cci_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_App",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
